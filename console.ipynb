{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd00d526aed0d0204b1c3266bfcf81d859f7d56c6eedf03f79ac782446919d26c6c",
   "display_name": "Python 3.9.2 64-bit ('image-captioning-env': venv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "data_folder = os.path.join(BASE_DIR, 'data')\n",
    "data_name = 'coco_5_cap_per_img_5_min_word_freq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "\n",
    "# temp dataset\n",
    "\n",
    "captions = []\n",
    "caplens = []\n",
    "\n",
    "captions_per_image = 5\n",
    "\n",
    "h5_file = h5py.File(os.path.join(data_folder, ('VAL_IMAGES_' + data_name + '.hdf5')), 'r')\n",
    "images = h5_file['images']\n",
    "\n",
    "with open(os.path.join(data_folder, ('VAL_CAPTIONS_' + data_name + '.json')), 'r') as json_file:\n",
    "    captions = json.load(json_file)\n",
    "\n",
    "# Load caption lengths (completely into memory)\n",
    "with open(os.path.join(data_folder, ('VAL_CAPLENS_' + data_name + '.json')), 'r') as json_file:\n",
    "    caplens = json.load(json_file)\n",
    "\n",
    "print(len(images))\n",
    "print(len(captions))\n",
    "print(len(caplens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 4000\n",
    "\n",
    "images = images[:data_size // 5]\n",
    "captions = captions[:data_size]\n",
    "caplens = caplens[:data_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_lens = set([len(i) for i in captions])\n",
    "captions_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "assert len(images) == 800\n",
    "assert len(captions) == 4000\n",
    "assert len(caplens) == 4000\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in tqdm(range(data_size)):\n",
    "    img = images[i // 5]\n",
    "    cp = captions[i]\n",
    "    clen = caplens[i]\n",
    "\n",
    "    data.append([img, cp, clen])\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, shuffle=False)\n",
    "\n",
    "print()\n",
    "print(len(train_data))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(train_data, 'data/train_data.pickle')\n",
    "joblib.dump(val_data, 'data/val_data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, raw_data, captions_per_image=5, mode='TRAIN'):\n",
    "        super(CaptionDataset, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.data_size = len(raw_data)\n",
    "        self.captions_per_image = captions_per_image\n",
    "        self.raw_images, self.raw_captions, self.raw_caplens = list(map(list, zip(*raw_data)))\n",
    "\n",
    "        self.images = torch.FloatTensor(self.raw_images)\n",
    "        self.captions = torch.LongTensor(self.raw_captions)\n",
    "        self.caplens = torch.LongTensor(self.raw_caplens)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = self.images[i]\n",
    "        caption = self.captions[i]\n",
    "        caplen = self.caplens[i]\n",
    "\n",
    "        if self.mode == 'TRAIN':\n",
    "            return img, caption, caplen\n",
    "        else:\n",
    "            # For validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\n",
    "            img_index = i // self.captions_per_image\n",
    "            all_captions = self.captions[img_index * 5:(img_index + 1) * 5]\n",
    "            return img, caption, caplen, all_captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "raw_train_data = joblib.load(os.path.join(data_folder, 'train_data.pickle'))\n",
    "raw_val_data = joblib.load(os.path.join(data_folder, 'val_data.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_dataset = CaptionDataset(raw_data=raw_train_data, mode='TRAIN')\n",
    "val_dataset = CaptionDataset(raw_data=raw_val_data, mode='VAL')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_loader))\n",
    "sample_img, sample_caption, sample_caplen = sample\n",
    "\n",
    "print(sample_img.shape)\n",
    "print(sample_caption.shape)\n",
    "print(sample_caplen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.model, self.vocab = get_pytorch_kobert_model()\n",
    "        self.sp = SentencepieceTokenizer(get_tokenizer())\n",
    "\n",
    "    def tokenize(self, sentence: str):\n",
    "        return self.sp(sentence)\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids: list):\n",
    "        return [self.vocab.idx_to_token[idx] for idx in ids]\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens: list):\n",
    "        return [self.vocab.token_to_idx[token] for token in tokens]\n",
    "\n",
    "    def get_word_map(self):\n",
    "        return self.vocab.token_to_idx\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab.token_to_idx)  # 7002\n",
    "\n",
    "    def get_embedding_dim(self):\n",
    "        return list(self.model.embeddings.children())[0].embedding_dim  # 768\n",
    "\n",
    "    def get_pretrained_embedding(self):\n",
    "        return self.model.embeddings.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet101\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoded_image_size=14):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_image_size = encoded_image_size\n",
    "\n",
    "        resnet = resnet101(pretrained=True)\n",
    "\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
    "        self.fine_tune()\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
    "        :return: encoded images\n",
    "        \"\"\"\n",
    "        # (batch_size, 2048, image_size/32, image_size/32)\n",
    "        out = self.resnet(images)\n",
    "        # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
    "        out = self.adaptive_pool(out)\n",
    "        # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "\n",
    "        # print('출력해보기_image_feature', out)\n",
    "        return out\n",
    "\n",
    "    def fine_tune(self, fine_tune=True):\n",
    "        \"\"\"\n",
    "         Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
    "\n",
    "         :param fine_tune: Allow?\n",
    "        \"\"\"\n",
    "        # parameter update되지 않도록 고정\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_feature_size, decoder_hidden_size, attention_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_feature_size = encoder_feature_size\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.attention_size = attention_size\n",
    "\n",
    "        self.encoder_att = nn.Linear(encoder_feature_size, attention_size)\n",
    "        self.decoder_att = nn.Linear(decoder_hidden_size, attention_size)\n",
    "        self.f_beta = nn.Linear(decoder_hidden_size, encoder_feature_size)\n",
    "        self.full_att = nn.Linear(attention_size, 1)\n",
    "\n",
    "    def forward(self, encoder_output, decoder_hidden):\n",
    "        # encoder_output : (batch_size, num_pixels, encoder_feature_size)\n",
    "        # decoder_hideen: (batch_size, decoder_hidden_size)\n",
    "        # (batch_size, num_pixels, attention_size)\n",
    "        att1 = self.encoder_att(encoder_output)\n",
    "        # (batch_size, attention_size)\n",
    "        att2 = self.decoder_att(decoder_hidden)\n",
    "\n",
    "        # att2.unsqueeze(1) -> (batch_size, 1, attention_size)\n",
    "        # att1 + att2.unsqueeze(1) -> (batch_size, num_pixels, attention_size)\n",
    "        # self.full_att(att1+att2.unsqueeze(1)) -> (batch_size, num_pixels, 1)\n",
    "        # (batch_size, num_pixels) <- 여기가 이해되지 X\n",
    "        att = self.full_att(F.relu(att1 + att2.unsqueeze(1))).squeeze(2)\n",
    "        alpha = F.softmax(att, dim=1)  # (batch_size, num_pixels)\n",
    "\n",
    "        # encoder_output : (batch_size, num_pixels, encoder_feature_size)\n",
    "        # alpha.unsqueeze(2) : (batch_size, num_pixels, 1)\n",
    "        # encoder_output*alpha.unsqueeze(2) -> (batch_size, num_pixels, encoder_feature_size)\n",
    "        attention_weighted_encoding = (encoder_output * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_feature_size)\n",
    "\n",
    "        gate = F.sigmoid(self.f_beta(decoder_hidden)) # [batch_size, enc_feature_size]\n",
    "\n",
    "        # hadamard product (gate and attention_weighted_encoding)\n",
    "        attention_weighted_encoding = gate * attention_weighted_encoding # [batch_size, enc_feature_size]\n",
    "\n",
    "        return attention_weighted_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, attention, tokenizer, hidden_size, dropout=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding_size = tokenizer.get_embedding_dim()\n",
    "        self.output_size = tokenizer.get_vocab_size()\n",
    "\n",
    "        self.embedding = tokenizer.get_pretrained_embedding()\n",
    "        self.attention = attention\n",
    "        self.lstm = nn.LSTM(self.embedding_size + self.attention.encoder_feature_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, self.output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, encoder_outputs, captions, hidden, cell):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        \n",
    "        attention_weights = self.attention(encoder_outputs, hidden)\n",
    "        \n",
    "        # embedding: [batch_size, 1, embedding_size]\n",
    "        # attention: [batch_size, encoder_feature_size]\n",
    "\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        cell = cell.unsqueeze(0)\n",
    "\n",
    "        # [batch_size, 1]\n",
    "        token = captions.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(token))\n",
    "        attn = attention_weights.unsqueeze(1)\n",
    "\n",
    "        lstm_input = torch.cat([embedded, attn], dim=2)\n",
    "        lstm_output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "\n",
    "        output = self.fc(lstm_output)\n",
    "\n",
    "        return output.squeeze(1), hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioner(nn.Module):\n",
    "    def __init__(self, tokenizer, encoder_feature_size, decoder_hidden_size, attention_size, dropout=0.5):\n",
    "        super(ImageCaptioner, self).__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.encoder_feature_size = encoder_feature_size\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.attention_size = attention_size\n",
    "\n",
    "        self.encoder = Encoder()\n",
    "        self.attention = Attention(encoder_feature_size, decoder_hidden_size, attention_size)\n",
    "        self.decoder = Decoder(self.attention, tokenizer, decoder_hidden_size)\n",
    "\n",
    "        self.init_h = nn.Linear(self.encoder_feature_size, self.decoder_hidden_size)\n",
    "        self.init_c = nn.Linear(self.encoder_feature_size, self.decoder_hidden_size)\n",
    "        \n",
    "    def forward(self, images, captions, caption_lengths):\n",
    "        batch_size = captions.size(0)\n",
    "        caption_length = captions.size(1)\n",
    "\n",
    "        # img -> encoder [batch_size, img_size, img_size, enc_feature_size]\n",
    "        encoder_outputs = self.encoder(images).view(batch_size, -1, encoder_feature_size)\n",
    "        num_pixels = encoder_outputs.size(1)\n",
    "\n",
    "        # Sort input data by decreasing lengths; why? apparent below\n",
    "        # caption_lengths.squeeze(1) -> (batch_size)\n",
    "        # caption length 별 내림차순 정렬\n",
    "        # 별 의미없는거같음\n",
    "        caption_lengths, sort_ind = caption_lengths.sort(dim=0, descending=True)\n",
    "        encoder_output = encoder_outputs[sort_ind]\n",
    "        encoded_caption = captions[sort_ind]\n",
    "\n",
    "        hidden, cell = self.init_hidden_state(encoder_output)  # (batch_size, decoder_hidden_size)\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "\n",
    "        # Create tensors to hold word predicion scores and alphas\n",
    "        vocab_size = tokenizer.get_vocab_size()\n",
    "        predictions = torch.zeros(batch_size, caption_length, vocab_size).to(device)\n",
    "\n",
    "        # At each time-step, decode by\n",
    "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
    "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
    "        for t in range(caption_length):\n",
    "            # batch_size_t = sum([l > t for l in decode_lengths])  # 학습할 데이터의 수\n",
    "            \n",
    "            # encoder_output_t = encoder_output[:batch_size_t]\n",
    "            decoder_output, _, _ = self.decoder(encoder_output, captions[:, t], hidden, cell)\n",
    "\n",
    "            # update predictions and alphas(=attention score)\n",
    "            predictions[:, t] = decoder_output\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def init_hidden_state(self, encoder_output):\n",
    "        mean = encoder_output.mean(dim=1)\n",
    "        hidden = self.init_h(mean)\n",
    "        cell = self.init_c(mean)\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# load pretrained word embeddings\n",
    "tokenizer = Tokenizer()\n",
    "embedding_weights = tokenizer.get_pretrained_embedding()\n",
    "\n",
    "# model parameters\n",
    "encoder_feature_size = 2048\n",
    "decoder_hidden_size = 1024\n",
    "attention_size = 512\n",
    "dropout = 0.5\n",
    "\n",
    "# hyper parameters\n",
    "num_epochs = 1\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = ImageCaptioner(tokenizer, encoder_feature_size, decoder_hidden_size, attention_size, dropout=dropout).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device='cpu'):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for _, (images, captions, caplens) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        caplens = caplens.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # output: [batch_size, cap_len, vocab_size]\n",
    "        # caption: [batch_size, cap_len]\n",
    "        output = model(images, captions, caplens)\n",
    "        output_dim = output.size(-1) # vocab_size\n",
    "\n",
    "        output = output.view(-1, output_dim) # [batch_size * cap_len, vocab_size]\n",
    "        captions = captions.view(-1) # [batch_size * cap_len]\n",
    "\n",
    "        loss = criterion(output, captions)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device='cpu'):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (img, caption, caplen) in enumerate(val_loader):\n",
    "            img = img.to(device)\n",
    "            caption = caption.to(device)\n",
    "            caplen = caplen.to(device)\n",
    "\n",
    "            output = model(img, caption, caplen)\n",
    "            output_dim = output.size(-1) # vocab_size\n",
    "\n",
    "            output = output.view(-1, output_dim) # [batch_size * cap_len, vocab_size]\n",
    "            captions = captions.view(-1) # [batch_size * cap_len]\n",
    "\n",
    "            loss = criterion(output, captions)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer)\n",
    "\n",
    "    val_loss = validate(model, val_loader, criterion)\n",
    "\n",
    "    print('Epoch [{02d}/{02d}] | Train Loss: {} | Val. Loss: {}'.format(epoch + 1, num_epochs, train_loss, val_loss))"
   ]
  }
 ]
}
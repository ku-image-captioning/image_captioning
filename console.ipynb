{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "console_resnet50.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KA0uOViKhyRJ",
        "xASHilTxTnQb",
        "fBGt_w1FT57Y",
        "q8wSgP6XULjt",
        "a4BIpM92fRGG",
        "LOvN7thofXt5",
        "rQ-pKmB0O5hP"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b477e15315a946f19ebe57aef78190c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9ad3ab102579401cb1ae2946b5362ec0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_368f71cc2e664fdab7b9f422c3ae7d38",
              "IPY_MODEL_4d36f6622fc74f70b0ce3ceb28fbaa49"
            ]
          }
        },
        "9ad3ab102579401cb1ae2946b5362ec0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "368f71cc2e664fdab7b9f422c3ae7d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_43e876fbb6864fc7a138dce68e7a7b21",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c00c61075489440093c6ce1624dd73ed"
          }
        },
        "4d36f6622fc74f70b0ce3ceb28fbaa49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_093c4bf1c1624351866030d2e712254b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:14&lt;00:00, 7.03MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0af0ea6e7b14e2c811693d95e1be98b"
          }
        },
        "43e876fbb6864fc7a138dce68e7a7b21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c00c61075489440093c6ce1624dd73ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "093c4bf1c1624351866030d2e712254b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0af0ea6e7b14e2c811693d95e1be98b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBzI9Y2LY7Wk"
      },
      "source": [
        "# 사용법\n",
        "1. 드라이브 연결\n",
        "2. Tokenizer - KoBERT 밑의 셀 실행\n",
        "3. 데이터 로드 -> Paths 밑에 압축파일 **경로 알아서 바꾸고** 압축해제 (사람마다 경로가 다를수있음)\n",
        "4. 나머지 셀 순서대로 실행\n",
        "\n",
        "## 중간에 끊겨도 상관없음!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaWDJc6bPjzc"
      },
      "source": [
        "# 0. Connect Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm3t5ijd3ug0",
        "outputId": "6a5d80e9-0088-4351-b7ff-824ca4b53940"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/image_caption'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je7PAczl55ny"
      },
      "source": [
        "# 1. Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6falmz66op3"
      },
      "source": [
        "### KoBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKfAZQYJGuKW",
        "outputId": "04dbb9de-ab69-40a0-edc5-f468d65303cc"
      },
      "source": [
        "# mxnet, gluonnlp, sentencepiece\n",
        "!pip install mxnet gluonnlp sentencepiece transformers\n",
        "\n",
        "# kobert\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/07/66174e78c12a3048db9039aaa09553e35035ef3a008ba3e0ed8d2aa3c47b/mxnet-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (46.9MB)\n",
            "\u001b[K     |████████████████████████████████| 46.9MB 98kB/s \n",
            "\u001b[?25hCollecting gluonnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/81/a238e47ccba0d7a61dcef4e0b4a7fd4473cb86bed3d84dd4fe28d45a0905/gluonnlp-0.10.0.tar.gz (344kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 35.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 31.7MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 32.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.19.5)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 34.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 33.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595677 sha256=c743306ba21a7598cc67f1ac2a3e641e8c9fe386c210d59b984f29040bb408a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/65/52/63032864a0f31a08b9a88569f803b5bafac8abd207fd7f7534\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: graphviz, mxnet, gluonnlp, sentencepiece, sacremoses, huggingface-hub, tokenizers, transformers\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed gluonnlp-0.10.0 graphviz-0.8.4 huggingface-hub-0.0.8 mxnet-1.8.0.post0 sacremoses-0.0.45 sentencepiece-0.1.95 tokenizers-0.10.3 transformers-4.6.1\n",
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-c2jfr3vb\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-c2jfr3vb\n",
            "Building wheels for collected packages: kobert\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.1.2-cp37-none-any.whl size=12708 sha256=e7316ff0c59e697a42d5304f4b65e2b51e54e9114cce39f6f85ba51d5a5afef1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xl1jsaj3/wheels/a2/b0/41/435ee4e918f91918be41529283c5ff86cd010f02e7525aecf3\n",
            "Successfully built kobert\n",
            "Installing collected packages: kobert\n",
            "Successfully installed kobert-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvObe1sw6oKy",
        "outputId": "521889fc-4f83-4f24-e439-c49f1fc82e3b"
      },
      "source": [
        "from gluonnlp.data import SentencepieceTokenizer\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "\n",
        "class KoBERTTokenizer:\n",
        "    def __init__(self):\n",
        "        self.type = 'KOBERT'\n",
        "        self.model, self.vocab = get_pytorch_kobert_model()\n",
        "        self.tokenizer = SentencepieceTokenizer(get_tokenizer())\n",
        "\n",
        "    def tokenize(self, sentence: str):\n",
        "        return self.tokenizer(sentence)\n",
        "\n",
        "    def idx_to_token(self, idx):\n",
        "        return self.vocab.idx_to_token[idx]\n",
        "\n",
        "    def token_to_idx(self, token):\n",
        "        return self.vocab.token_to_idx[token]\n",
        "\n",
        "    def get_word_map(self):\n",
        "        return self.vocab.token_to_idx\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return len(self.vocab.token_to_idx)  # 7002\n",
        "\n",
        "    def get_embedding_dim(self):\n",
        "        return list(self.model.embeddings.children())[0].embedding_dim  # 768\n",
        "\n",
        "    def get_pretrained_embedding(self):\n",
        "        return self.model.embeddings.word_embeddings\n",
        "\n",
        "\n",
        "tokenizer = KoBERTTokenizer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n",
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqcD-Ond5zSJ"
      },
      "source": [
        "# 2. 데이터 로드\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEcSvt8ZPcOc"
      },
      "source": [
        "## Paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F3uEC-fweAq"
      },
      "source": [
        "# Sample data (12,000 images)\n",
        "# !unzip -qq /content/drive/MyDrive/image_caption/data/train2014.zip\n",
        "# !unzip -qq /content/drive/MyDrive/image_caption/data/val2014.zip\n",
        "\n",
        "# Full-size data (120,000 images)\n",
        "!tar -zxf /content/drive/MyDrive/image_caption/data/train2014.tar.gz\n",
        "!tar -zxf /content/drive/MyDrive/image_caption/data/val2014.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1JGqyu23fG_"
      },
      "source": [
        "import os\n",
        "\n",
        "# BASE_DIR = os.getcwd()\n",
        "\n",
        "# 내 폴더구조에 알아서 맞게 사용하기 !!!\n",
        "BASE_DIR = '/content/drive/MyDrive/image_caption'\n",
        "\n",
        "data_folder = os.path.join(BASE_DIR, 'data')\n",
        "\n",
        "# root_dir = \"/content/drive/MyDrive/0.졸업프로젝트_공유/1.data\"\n",
        "tokenizer_dir = os.path.join(BASE_DIR, 'tokenizer')\n",
        "\n",
        "# model_dir = os.path.join(tokenizer_dir, \"model\")\n",
        "komoran_dict = os.path.join(tokenizer_dir, \"userdict.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDoQvC5lYqbK"
      },
      "source": [
        "komoran_w2v_model_path = os.path.join(tokenizer_dir, \"KOMORAN_W2V.model\")\n",
        "okt_w2v_model_path = os.path.join(tokenizer_dir, \"OKT_W2V.model\")\n",
        "mecab_w2v_model_path = os.path.join(tokenizer_dir, \"MECAB_W2V.model\")\n",
        "\n",
        "komoran_glove_model_path = os.path.join(tokenizer_dir, \"KOMORAN_GLOVE_COLAB.model\")\n",
        "okt_glove_model_path = os.path.join(tokenizer_dir, \"OKT_GLOVE_COLAB.model\")\n",
        "mecab_glove_model_path = os.path.join(tokenizer_dir, \"MECAB_GLOVE.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfgvCQnYPeIW"
      },
      "source": [
        "## Dataset Class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vzNHDsa3fHH"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image, ImageFile\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "class CaptionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_folder, mode):\n",
        "        super(CaptionDataset, self).__init__()\n",
        "        \n",
        "        if mode not in ['TRAIN', 'VAL']:\n",
        "            raise ValueError(\n",
        "                '\"mode\" must be either \"TRAIN\" or \"VAL\". Got \"{}\".\"'.format(mode))\n",
        "\n",
        "        self.data_folder = data_folder\n",
        "        self.caption_per_image = 5\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        filename = '{}_data.json'.format(mode.lower())\n",
        "        with open(os.path.join(data_folder, filename), 'r') as fp:\n",
        "            raw_data = json.load(fp)\n",
        "\n",
        "        self.images = raw_data['images']\n",
        "        self.captions = raw_data['captions']\n",
        "        self.caplens = raw_data['caplens']\n",
        "        self.all_captions = raw_data['all_captions']\n",
        "\n",
        "        self.data_size = len(self.images)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        image_path = os.path.join('data', self.images[i])\n",
        "        image = self.transform(Image.open(image_path).convert('RGB'))  \n",
        "        caption = self.captions[i]\n",
        "        caplen = self.caplens[i]\n",
        "        all_captions = self.all_captions[i]\n",
        "\n",
        "        caption = torch.LongTensor(caption)\n",
        "        caplen = torch.LongTensor([caplen])\n",
        "        all_caption = torch.LongTensor(all_captions)\n",
        "\n",
        "        return image, caption, caplen, all_caption\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMRyOj6Ucq3a"
      },
      "source": [
        "## 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_3Pcthw3fHH"
      },
      "source": [
        "batch_size = 32\n",
        "num_workers = 4\n",
        "\n",
        "train_dataset = CaptionDataset(data_folder, 'TRAIN')\n",
        "val_dataset = CaptionDataset(data_folder, 'VAL')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4BIpM92fRGG"
      },
      "source": [
        "# 3. 모델링\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jerwaL5j45Dc"
      },
      "source": [
        "## 3.1. Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-h-Jf283fHI"
      },
      "source": [
        "from torchvision.models import resnet50\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, encoded_image_size=14):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "        resnet = resnet50(pretrained=True)\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
        "        self.fine_tune()\n",
        "\n",
        "    def forward(self, images):\n",
        "        # (batch_size, 2048, image_size/32, image_size/32)\n",
        "        out = self.resnet(images)\n",
        "        # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
        "        out = self.adaptive_pool(out)\n",
        "        # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
        "        out = out.permute(0, 2, 3, 1)\n",
        "        return out\n",
        "\n",
        "    def fine_tune(self, fine_tune=True):\n",
        "        # parameter update되지 않도록 고정\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
        "        for c in list(self.resnet.children())[5:]:\n",
        "            for p in c.parameters():\n",
        "                p.requires_grad = fine_tune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZeQdtcy47zM"
      },
      "source": [
        "## 3.2. Attention Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6i6-nai3fHJ"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_feature_size, decoder_hidden_size, attention_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_feature_size = encoder_feature_size    #feature size of encoded img\n",
        "        self.decoder_hidden_size = decoder_hidden_size  #size of decoders RNN\n",
        "        self.attention_size = attention_size\n",
        "\n",
        "        self.encoder_att = nn.Linear(encoder_feature_size, attention_size)  #transform encoded img\n",
        "        self.decoder_att = nn.Linear(decoder_hidden_size, attention_size)   #transform decoders output\n",
        "        self.f_beta = nn.Linear(decoder_hidden_size, encoder_feature_size)  #create a sigmoid-activated gate\n",
        "        self.full_att = nn.Linear(attention_size, 1)    #caculate values to be softmax-ed\n",
        "\n",
        "    def forward(self, encoder_output, decoder_hidden):\n",
        "        # encoder_output : (batch_size, num_pixels, encoder_feature_size)\n",
        "        # decoder_hideen: (batch_size, decoder_hidden_size)\n",
        "        # (batch_size, num_pixels, attention_size)\n",
        "        att1 = self.encoder_att(encoder_output)\n",
        "        # (batch_size, attention_size)\n",
        "        att2 = self.decoder_att(decoder_hidden)\n",
        "\n",
        "        # att2.unsqueeze(1) -> (batch_size, 1, attention_size)\n",
        "        # att1 + att2.unsqueeze(1) -> (batch_size, num_pixels, attention_size)\n",
        "        # self.full_att(att1+att2.unsqueeze(1)) -> (batch_size, num_pixels, 1)\n",
        "        att = self.full_att(F.relu(att1 + att2.unsqueeze(1))).squeeze(2)\n",
        "        alpha = F.softmax(att, dim=1)  # (batch_size, num_pixels)\n",
        "\n",
        "        # encoder_output : (batch_size, num_pixels, encoder_feature_size)\n",
        "        # alpha.unsqueeze(2) : (batch_size, num_pixels, 1)\n",
        "        # encoder_output*alpha.unsqueeze(2) -> (batch_size, num_pixels, encoder_feature_size)\n",
        "        attention_weighted_encoding = (encoder_output * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_feature_size)\n",
        "\n",
        "        gate = torch.sigmoid(self.f_beta(decoder_hidden)) # [batch_size, enc_feature_size]\n",
        "\n",
        "        # hadamard product (gate and attention_weighted_encoding)\n",
        "        attention_weighted_encoding = gate * attention_weighted_encoding # [batch_size, enc_feature_size]\n",
        "\n",
        "        return attention_weighted_encoding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQcFo_9A4_DX"
      },
      "source": [
        "## 3.3. Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpMLmFjZ3fHJ"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, attention, tokenizer, hidden_size, dropout=0.5):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding_size = tokenizer.get_embedding_dim()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = tokenizer.get_vocab_size()\n",
        "\n",
        "        self.embedding = tokenizer.get_pretrained_embedding()\n",
        "        self.attention = attention\n",
        "        self.lstm = nn.LSTM(\n",
        "            self.embedding_size + self.attention.encoder_feature_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, self.output_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, encoder_outputs, captions, hidden, cell):\n",
        "        attention_weights = self.attention(encoder_outputs, hidden)\n",
        "        \n",
        "        # embedding: [batch_size, 1, embedding_size]\n",
        "        # attention: [batch_size, encoder_feature_size]\n",
        "\n",
        "        hidden = hidden.unsqueeze(0)\n",
        "        cell = cell.unsqueeze(0)\n",
        "\n",
        "        # [batch_size, 1]\n",
        "        token = captions.unsqueeze(1)\n",
        "        embedded = self.dropout(self.embedding(token))\n",
        "        attn = attention_weights.unsqueeze(1)\n",
        "\n",
        "        lstm_input = torch.cat([embedded, attn], dim=2)\n",
        "        lstm_output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "\n",
        "        output = self.fc(lstm_output)\n",
        "\n",
        "        return output.squeeze(1), hidden.squeeze(0), cell.squeeze(0), attention_weights\n",
        "\n",
        "    def init_hidden(self, batch_size, device='cpu'):\n",
        "        hidden = torch.zeros(batch_size, self.hidden_size, device=device)\n",
        "        cell = torch.zeros(batch_size, self.hidden_size, device=device)\n",
        "        return hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gybPqQiT5BCI"
      },
      "source": [
        "## 3.4. ImageCaptioner - Integrated Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qa3Bxg_3fHK"
      },
      "source": [
        "import torch\n",
        "import random\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "\n",
        "class ImageCaptioner(nn.Module):\n",
        "    def __init__(self, tokenizer, encoder_feature_size, decoder_hidden_size, attention_size, dropout=0.5):\n",
        "        super(ImageCaptioner, self).__init__()\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.encoder_feature_size = encoder_feature_size\n",
        "        self.decoder_hidden_size = decoder_hidden_size\n",
        "        self.attention_size = attention_size\n",
        "\n",
        "        self.encoder = Encoder(encoded_image_size=16).to(self.device)\n",
        "        self.attention = Attention(encoder_feature_size, decoder_hidden_size, attention_size).to(self.device)\n",
        "        self.decoder = Decoder(self.attention, tokenizer, decoder_hidden_size, dropout=dropout).to(self.device)\n",
        "\n",
        "    def forward(self, images, captions, caption_lengths, teacher_forcing_ratio=0.5):\n",
        "        # images: 3 * 256 * 256 으로 인코딩된거 -> [batch_size, 3, 256, 256]    \n",
        "        # captions: 인코딩되고 패딩까지 추가된 토큰들 -> [batch_size, cap_len]\n",
        "        # caption_lengths: 패딩이 추가안된 순수 문장 길이들 -> [batch_size, 1]\n",
        "        batch_size, cap_len = captions.size()\n",
        "\n",
        "        # img -> encoder [batch_size, img_size, img_size, enc_feature_size]\n",
        "        encoder_outputs = self.encoder(images).reshape(batch_size, -1, encoder_feature_size)\n",
        "        num_pixels = encoder_outputs.size(1)\n",
        "\n",
        "        # caption length 별 내림차순 정렬\n",
        "        caption_lengths = caption_lengths.squeeze(1)  # [batch_size]\n",
        "        caption_lengths, sort_ind = caption_lengths.sort(dim=0, descending=True)\n",
        "        encoder_outputs = encoder_outputs[sort_ind]\n",
        "        captions = captions[sort_ind]\n",
        "\n",
        "        hidden, cell = self.decoder.init_hidden(batch_size, device=self.device)  # [batch_size, decoder_hidden_size]\n",
        "        decode_lengths = caption_lengths.tolist()\n",
        "        \n",
        "        # pack_padded_sequence: <PAD> 토큰들을 죄다 지워줌!!\n",
        "        # batch_sizes: <PAD> 토큰을 지우니깐 문장 길이가 달라짐 \n",
        "        # -> 디코더 스텝별로 들어가는 데이터 길이가 달라지니깐 \n",
        "        # -> 스텝마다 몇개가 들어가는지 리턴!!!\n",
        "        captions_packed = pack_padded_sequence(captions, decode_lengths, batch_first=True)\n",
        "        captions_packed_data = captions_packed.data\n",
        "        batch_sizes = captions_packed.batch_sizes\n",
        "\n",
        "        # 예측값 저장하는 tensor를 만듦\n",
        "        vocab_size = tokenizer.get_vocab_size()\n",
        "        predictions = torch.zeros(batch_size, cap_len, vocab_size, device=self.device)\n",
        "\n",
        "        # 초기값\n",
        "        index = 0\n",
        "        batch_size = batch_sizes[0]\n",
        "        decoder_input = captions_packed_data[:batch_size]\n",
        "\n",
        "        num_batch_sizes = len(batch_sizes)\n",
        "\n",
        "        for t in range(1, num_batch_sizes):\n",
        "            encoder_outputs = encoder_outputs[:batch_size]\n",
        "            hidden = hidden[:batch_size]\n",
        "            cell = cell[:batch_size]\n",
        "\n",
        "            decoder_output, hidden, cell, _ = self.decoder(encoder_outputs, decoder_input, hidden, cell)\n",
        "            predictions[:batch_size, t] = decoder_output\n",
        "\n",
        "            # 다음 스텝\n",
        "            batch_size = batch_sizes[t]\n",
        "            index += batch_size\n",
        "\n",
        "            # 지도학습\n",
        "            teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "            decoder_input = captions_packed_data[index:index + batch_size] if teacher_forcing else decoder_output.argmax(1)[:batch_size]\n",
        "\n",
        "        # predictions: 예측값 (전체 단어들에 대한 확률들 싹 다) -> [batch_size, cap_len, vocab_size]\n",
        "        # captions: 정렬된 정답들\n",
        "        # decode_lengths: 정렬된 문장 길이들\n",
        "        # sort_ind: 원본에서의 순서 [2, 4, 1, 3, 5] => [4, 1, 3, 0, 2]\n",
        "        return predictions, captions[:, 1:], decode_lengths, sort_ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-khw-zs5O94"
      },
      "source": [
        "## 3.5. 모델 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "b477e15315a946f19ebe57aef78190c2",
            "9ad3ab102579401cb1ae2946b5362ec0",
            "368f71cc2e664fdab7b9f422c3ae7d38",
            "4d36f6622fc74f70b0ce3ceb28fbaa49",
            "43e876fbb6864fc7a138dce68e7a7b21",
            "c00c61075489440093c6ce1624dd73ed",
            "093c4bf1c1624351866030d2e712254b",
            "a0af0ea6e7b14e2c811693d95e1be98b"
          ]
        },
        "id": "DifC-jBO3fHK",
        "outputId": "8323abc7-f9f4-484c-9dff-1fe13b0bd8a6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# model parameters\n",
        "encoder_feature_size = 2048\n",
        "decoder_hidden_size = 1024\n",
        "attention_size = 512\n",
        "dropout = 0.5\n",
        "learning_rate = 0.001\n",
        "# decay_rate = 0.9\n",
        "\n",
        "model = ImageCaptioner(tokenizer, encoder_feature_size, decoder_hidden_size, attention_size, dropout=dropout).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b477e15315a946f19ebe57aef78190c2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOvN7thofXt5"
      },
      "source": [
        "# 4. 학습 및 검증"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GNFF1WD5U3t"
      },
      "source": [
        "## 4.1. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Drlb2Vi3fHL"
      },
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, device='cpu'):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    # scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for i, (images, captions, caplens, _) in enumerate(tqdm(train_loader)):        \n",
        "        images = images.to(device)\n",
        "        captions = captions.to(device)\n",
        "        caplens = caplens.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # with autocast():\n",
        "        # predictions: [batch_size, cap_len, vocab_size]\n",
        "        # caption: [batch_size, cap_len]\n",
        "        predictions, captions, caplens, sort_ind = model(\n",
        "            images, \n",
        "            captions, \n",
        "            caplens, \n",
        "            teacher_forcing_ratio=0.5\n",
        "        )\n",
        "\n",
        "        predictions = pack_padded_sequence(predictions, caplens, batch_first=True).data\n",
        "        captions = pack_padded_sequence(captions, caplens, batch_first=True).data\n",
        "\n",
        "        loss = criterion(predictions, captions)\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # scaler.scale(loss).backward()\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient Clipping        \n",
        "        # scaler.unscale_(optimizer)\n",
        "        clip_grad_norm_(model.parameters(), 1)\n",
        "\n",
        "        # scaler.step(optimizer)\n",
        "        # scaler.update()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    return epoch_loss / len(train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUdwIvdahZcC"
      },
      "source": [
        "## 4.2. Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2wxnWOZ3fHL"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "\n",
        "\n",
        "def validate(model, val_loader, criterion, device='cpu'):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, (images, captions, caplens, _) in enumerate(tqdm(val_loader)):\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "            caplens = caplens.to(device)\n",
        "\n",
        "            predictions, captions, caplens, sort_ind = model(\n",
        "                images, captions, caplens, teacher_forcing_ratio=0)\n",
        "    \n",
        "            preds = pack_padded_sequence(predictions, caplens, batch_first=True).data\n",
        "            captions = pack_padded_sequence(captions, caplens, batch_first=True).data\n",
        "\n",
        "            loss = criterion(preds, captions)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    epoch_loss /= len(val_loader)  \n",
        "\n",
        "    return epoch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgZOYGKg5nzn"
      },
      "source": [
        "## 4.3. 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ5Ht3DI5hwQ"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "from torch.autograd import profiler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "model_dir = os.path.join(BASE_DIR, 'output')\n",
        "checkpoint_path = os.path.join(model_dir, 'training-model-resnet50.pt')\n",
        "model_path = os.path.join(model_dir, 'savepoint-resnet50.pt')\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# Train Parameters\n",
        "num_epochs = 100\n",
        "patience = 20\n",
        "early_stop_counting = 0\n",
        "\n",
        "try:\n",
        "    # Load Checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    # Load model and optimizer\n",
        "    model.load_state_dict(checkpoint.get('model_state'))\n",
        "    optimizer.load_state_dict(checkpoint.get('optimizer'))\n",
        "    # Load epoch and losses\n",
        "    start_epoch = checkpoint.get('epoch')\n",
        "    train_loss = checkpoint.get('train_loss')\n",
        "    val_loss = checkpoint.get('val_loss')\n",
        "\n",
        "    if not val_loss:\n",
        "        val_loss = float('inf')\n",
        "except (FileNotFoundError, RuntimeError):\n",
        "    start_epoch = 0\n",
        "    train_loss = float('inf')\n",
        "    val_loss = float('inf')\n",
        "\n",
        "best_valid_loss = val_loss\n",
        "\n",
        "print('Resume training from {} epoch, {:.4f} train loss, {:.4f} valid loss\\n'.format(start_epoch, train_loss, val_loss))\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, device=device)\n",
        "    state_dict = {\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'train_loss': train_loss,\n",
        "    }\n",
        "    torch.save(state_dict, checkpoint_path)\n",
        "\n",
        "    val_loss = validate(model, val_loader, criterion, device=device)\n",
        "    state_dict = {\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'train_loss': train_loss,\n",
        "        'val_loss': val_loss,\n",
        "    }\n",
        "    torch.save(state_dict, checkpoint_path)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(\n",
        "        'Epoch: [{:02d}/{:02d}] | Time: {:02d}m {:02d}s | Train Loss: {:.4f} | Val. Loss: {:.4f} |' \\\n",
        "            .format(epoch + 1, num_epochs, epoch_mins, epoch_secs, train_loss, val_loss), \n",
        "        end=' '\n",
        "    )\n",
        "\n",
        "    if val_loss < best_valid_loss:\n",
        "        # save model output state\n",
        "        best_valid_loss = val_loss\n",
        "        state_dict = {\n",
        "            'encoder': model.encoder.state_dict(),\n",
        "            'decoder': model.decoder.state_dict(),\n",
        "        }\n",
        "        torch.save(state_dict, model_path)\n",
        "        print('| Improvement!')\n",
        "    else:\n",
        "        early_stop_counting += 1\n",
        "        print('|')\n",
        "\n",
        "    if early_stop_counting >= patience:\n",
        "        print('Early Stopping')\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLPPnEyut9eE"
      },
      "source": [
        "## 4.4. 검증"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC2P-QBHt_uA",
        "outputId": "3321b294-03bd-47ab-f571-218d3fac2cab"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "\n",
        "def test(model, test_dataset, test_data_size, device='cpu'):\n",
        "    model.eval()\n",
        "\n",
        "    candidates = []\n",
        "    references = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(test_data_size):\n",
        "            images, captions, caplens, all_captions = test_dataset[i]\n",
        "            images = images.unsqueeze(0).to(device)\n",
        "            captions = captions.unsqueeze(0).to(device)\n",
        "            caplens = caplens.unsqueeze(0).to(device)\n",
        "\n",
        "            predictions, captions, caplens, sort_ind = model(\n",
        "                images, captions, caplens, teacher_forcing_ratio=0)\n",
        "\n",
        "            _, top1 = torch.max(predictions, dim=2)\n",
        "            top1 = top1.squeeze(0)\n",
        "\n",
        "            stop_words = ['[UNK]', '[CLS]', '[SEP]', '[PAD]']\n",
        "            stop_words_idx = [tokenizer.token_to_idx(token) for token in stop_words]\n",
        "\n",
        "            # convert top1(우리가 생성한 토큰) to tokens: Candidate\n",
        "            prediction_translated = [\n",
        "                tokenizer.idx_to_token(idx) for idx in top1\n",
        "                if idx not in stop_words_idx\n",
        "            ]\n",
        " \n",
        "            # convert all_captions(이미지에 대한 캡션 5개) to tokens: References\n",
        "            all_captions_translated = [\n",
        "                [\n",
        "                    tokenizer.idx_to_token(idx) for idx in caption\n",
        "                    if idx not in stop_words_idx\n",
        "                ] \n",
        "                for caption in all_captions\n",
        "            ]\n",
        " \n",
        "            candidates.append(prediction_translated)\n",
        "            references.append(all_captions_translated)\n",
        "\n",
        "    return corpus_bleu(references, candidates, emulate_multibleu=True)\n",
        "\n",
        "print('BLEU Score: {}'.format(10 * test(model, val_dataset, 100, device=device)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU Score: 19.34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ-pKmB0O5hP"
      },
      "source": [
        "# 5. 프로파일링"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDy1IF9yO5B1"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.profiler as profiler\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "images = torch.rand((32, 3, 256, 256), device=device)\n",
        "captions = torch.randint(32, (32, 64), device=device)\n",
        "caplens = torch.randint(1, 64, (32, 1), device=device)\n",
        "\n",
        "profile_dir = os.path.join(BASE_DIR, 'profile')\n",
        "\n",
        "with profiler.profile(\n",
        "    activities=[\n",
        "        profiler.ProfilerActivity.CPU, \n",
        "        profiler.ProfilerActivity.CUDA\n",
        "    ],\n",
        "    with_stack=True, \n",
        "    record_shapes=True, \n",
        "    profile_memory=True\n",
        ") as prof:\n",
        "    predictions, captions, caplens, sort_ind = model(\n",
        "        images, \n",
        "        captions, \n",
        "        caplens, \n",
        "        teacher_forcing_ratio=0.5\n",
        "    )\n",
        "\n",
        "print(prof.key_averages(group_by_stack_n=10).table(sort_by='self_cuda_time_total'))\n",
        "prof.export_chrome_trace(os.path.join(profile_dir, 'profile-resnet50.pt.trace.json'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSpaKB-YSZvr"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '/content/drive/MyDrive/image_caption/profile'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQzRnFQ9RNsw"
      },
      "source": [
        "# 6. 실전 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdGC_IC9RRBp"
      },
      "source": [
        "import operator\n",
        "from queue import PriorityQueue\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class BeamSearchNode:\n",
        "    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
        "        '''\n",
        "        :param hiddenstate:\n",
        "        :param previousNode:\n",
        "        :param wordId:\n",
        "        :param logProb:\n",
        "        :param length:\n",
        "        '''\n",
        "        self.h = hiddenstate\n",
        "        self.prevNode = previousNode\n",
        "        self.wordid = wordId\n",
        "        self.logp = logProb\n",
        "        self.leng = length\n",
        "\n",
        "    def eval(self, alpha=1.0):\n",
        "        reward = 0\n",
        "        return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward\n",
        "\n",
        "\n",
        "def predict_with_beam_search(input_data, model_class, model_path, tokenizer):\n",
        "    # input_data: 하나의 이미지 인풋 (imread 로 읽어들인)!\n",
        "    # model_class: 모델 클래스\n",
        "    # model_path: 우리가 모델을 저장한 곳!\n",
        "    encoder = model_class.encoder\n",
        "    decoder = model_class.decoder\n",
        "\n",
        "    checkpoint = torch.load(model_path, map_location='cpu')\n",
        "    encoder.load_state_dict(checkpoint['encoder'])\n",
        "    decoder.load_state_dict(checkpoint['decoder'])\n",
        "\n",
        "    model_class.eval()\n",
        "\n",
        "    input_data = input_data.to(device)\n",
        "    encoder_outputs = encoder(input_data.unsqueeze(0)).view(1, -1, encoder_feature_size)\n",
        "\n",
        "    # start symbol\n",
        "    start_symbol = [tokenizer.token_to_idx('[CLS]')]\n",
        "    decoder_input = torch.tensor(start_symbol).to(device)\n",
        "    hidden = torch.zeros(1, decoder_hidden_size).to(device)\n",
        "    cell = torch.zeros(1, decoder_hidden_size).to(device)\n",
        "\n",
        "    max_length = 50\n",
        "    decoded_batch = []\n",
        "    attention_weights = []\n",
        "\n",
        "    # Number of sentence to generate\n",
        "    beam_width = 10\n",
        "    topk = 1  # how many sentence do you want to generate\n",
        "\n",
        "    end_nodes = []\n",
        "    number_required = min((topk + 1), topk - len(end_nodes))\n",
        "\n",
        "    # starting node -  hidden vector, previous node, word id, logp, length\n",
        "    node = BeamSearchNode(hidden, None, decoder_input, 0, 1)\n",
        "    nodes = PriorityQueue()\n",
        "\n",
        "    # start the queue\n",
        "    nodes.put((-node.eval(), node))\n",
        "    qsize = 1\n",
        "\n",
        "    for i in range(max_length):\n",
        "        if qsize > 2000:\n",
        "            break\n",
        "\n",
        "        # fetch the best node\n",
        "        score, n = nodes.get()\n",
        "        decoder_input = n.wordid\n",
        "        decoder_hidden = n.h\n",
        "\n",
        "        if n.wordid.item() == tokenizer.token_to_idx('[SEP]') and n.prevNode != None:\n",
        "            end_nodes.append((score, n))\n",
        "            # if we reached maximum # of sentences required\n",
        "            if len(end_nodes) >= number_required:\n",
        "                break\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        output, hidden, cell, attn = decoder(encoder_outputs, decoder_input, hidden, cell)\n",
        "        attention_weights.append(attn.detach().cpu().numpy())\n",
        "\n",
        "        # PUT HERE REAL BEAM SEARCH OF TOP\n",
        "        log_prob, indices = torch.topk(output, beam_width)\n",
        "        next_nodes = []\n",
        "\n",
        "        for new_k in range(beam_width):\n",
        "            decoded_t = indices[0][new_k].unsqueeze(0)\n",
        "            log_p = log_prob[0][new_k].item()\n",
        "\n",
        "            node = BeamSearchNode(decoder_hidden, n, decoded_t, n.logp + log_p, n.leng + 1)\n",
        "            score = -node.eval()\n",
        "            next_nodes.append((score, node))\n",
        "\n",
        "        # put them into queue\n",
        "        for i in range(len(next_nodes)):\n",
        "            score, nn = next_nodes[i]\n",
        "            nodes.put((score, nn))\n",
        "        \n",
        "        # increase qsize\n",
        "        qsize += len(next_nodes) - 1\n",
        "\n",
        "    # choose nbest paths, back trace them\n",
        "    if len(end_nodes) == 0:\n",
        "        end_nodes = [nodes.get() for _ in range(topk)]\n",
        "\n",
        "    for score, n in sorted(end_nodes, key=operator.itemgetter(0)):\n",
        "        tokens = []\n",
        "        tokens.append(n.wordid.item())\n",
        "        \n",
        "        # back trace\n",
        "        while n.prevNode != None:\n",
        "            n = n.prevNode\n",
        "            tokens.append(n.wordid.item())\n",
        "\n",
        "        tokens = tokens[::-1]\n",
        "        \n",
        "        stop_words = [tokenizer.token_to_idx('[CLS]'), tokenizer.token_to_idx('[SEP]')]\n",
        "        sentence = [tokenizer.idx_to_token(idx) for idx in tokens if idx not in stop_words]\n",
        "\n",
        "        decoded_batch.append(sentence)\n",
        "\n",
        "    return decoded_batch, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5Abb_bjRajP"
      },
      "source": [
        "def predict_with_greedy(input_data, model_class, model_path, tokenizer):\n",
        "    # input_data: 하나의 이미지 인풋 (imread 로 읽어들인)!\n",
        "    # model_class: 모델 클래스\n",
        "    # model_path: 우리가 모델을 저장한 곳!\n",
        "    encoder = model_class.encoder\n",
        "    decoder = model_class.decoder\n",
        "\n",
        "    checkpoint = torch.load(model_path, map_location='cpu')\n",
        "    encoder.load_state_dict(checkpoint['encoder'])\n",
        "    decoder.load_state_dict(checkpoint['decoder'])\n",
        "\n",
        "    model_class.eval()\n",
        "\n",
        "    input_data = input_data.to(device)\n",
        "    encoder_outputs = encoder(input_data.unsqueeze(0)).view(1, -1, encoder_feature_size)\n",
        "\n",
        "    # start symbol\n",
        "    start_symbol = [tokenizer.token_to_idx('[CLS]')]\n",
        "    decoder_input = torch.tensor(start_symbol).to(device)\n",
        "    hidden = torch.zeros(1, decoder_hidden_size).to(device)\n",
        "    cell = torch.zeros(1, decoder_hidden_size).to(device)\n",
        "\n",
        "    max_length = 50\n",
        "    tokens = []\n",
        "    attention_weights = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        output, hidden, cell, attn = decoder(encoder_outputs, decoder_input, hidden, cell)\n",
        "        top1 = output.argmax(1)\n",
        "        decoder_input = top1\n",
        "\n",
        "        token = tokenizer.idx_to_token(top1)\n",
        "        if token == '[SEP]':\n",
        "            break\n",
        "\n",
        "        tokens.append(token)\n",
        "        attention_weights.append(attn.detach().cpu().numpy())\n",
        "\n",
        "    return [tokens], attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bT2_jkWlY0l"
      },
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install fonts-nanum* -qq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izu4g2MsRcZu"
      },
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as mfm\n",
        "\n",
        "\n",
        "def plot_attention(image, tokens, attention_weights):\n",
        "    image = image.permute(1, 2, 0)  # [height, width, channels]\n",
        "    \n",
        "    num_elements = len(tokens)\n",
        "    num_cols = 5\n",
        "    num_rows = math.ceil(num_elements / num_cols)\n",
        "\n",
        "    figure = plt.figure(figsize=(20, 20))\n",
        "    prop = mfm.FontProperties(fname='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf', size=14)\n",
        "\n",
        "    for i in range(num_elements):\n",
        "        temp_att = np.resize(attention_weights[i], (16, 16))\n",
        "        ax = figure.add_subplot(num_rows, num_cols, i + 1)\n",
        "        ax.set_title(tokens[i], fontproperties=prop)\n",
        "        img = ax.imshow(image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d3SWJGjReE-"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# model parameters\n",
        "encoder_feature_size = 2048\n",
        "decoder_hidden_size = 1024\n",
        "attention_size = 512\n",
        "dropout = 0.5\n",
        "\n",
        "model = ImageCaptioner(tokenizer, encoder_feature_size, decoder_hidden_size, attention_size, dropout=dropout).to(device)\n",
        "\n",
        "BASE_DIR = os.getcwd()\n",
        "sample_image_folder = os.path.join(BASE_DIR, 'data', 'sample')\n",
        "sample_images = glob.glob(os.path.join(sample_image_folder, '*'))\n",
        "model_path = os.path.join(BASE_DIR, 'output', 'model', 'savepoint-resnet50.pt')\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# [channels, height, width]\n",
        "images = [transform(Image.open(img).convert('RGB')) for img in sample_images]\n",
        "num_images = len(images)\n",
        "\n",
        "for i in range(num_images):\n",
        "    tokens, attn_weights = predict_with_greedy(images[i], model, model_path, tokenizer)\n",
        "    plot_attention(images[i], tokens[0], attn_weights)\n",
        "\n",
        "    tokens, attn_weights = predict_with_beam_search(images[i], model, model_path, tokenizer)\n",
        "    plot_attention(images[i], tokens[0], attn_weights)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}